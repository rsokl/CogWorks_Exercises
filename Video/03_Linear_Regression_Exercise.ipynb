{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d98237d",
   "metadata": {},
   "source": [
    "# Exercises: Fitting a Linear Model with Gradient Descent\n",
    "\n",
    "This notebook is a very important one.\n",
    "We will return to our favorite problem: modeling the relationship between an NBA player's height and his wingspan using a linear model. \n",
    "\n",
    "Before, we found that we could exactly solve for the model parameters (i.e. the slope and y-intercept) that minimize the squared-residuals between our recorded data and our model's predictions. Now, we will act as if no such analytic solution exists, since this will almost always be the case in \"real world\" problems. Instead, we will use gradient descent to tune the parameters of our linear model, and we will do this by leveraging MyGrad's autodiff capabilities to compute the relevant gradients for this optimization process. The procedure that we exercise here will turn out to be almost exactly identical to the process for \"training a neural network\" using \"supervised learning\", which are concepts that we will dive into ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import the necessary libraries\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import mygrad as mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NetCDF-4 file `./data/nba_draft_measurements.nc` as an xarray-dataset\n",
    "# (refer to the previous exercise notebook if you need a refresher on this)\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot that shows wingspan versus height (without shoes) for the players\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cfeb1",
   "metadata": {},
   "source": [
    "## Reviewing Our Modeling Problem\n",
    "\n",
    "Based on the relationship between height and wingspan that we visualized above, we want to define a **linear mathematical model that can predict an individual's wingspan based on their height**.\n",
    "\n",
    "Our recorded data consists of $N$ measurements: $\\big(x_n, y^{\\mathrm{(true)}}_n\\big)_{n=0}^{N-1}$.\n",
    "Datum-$i$, $\\big(x_i, y^{\\mathrm{(true)}}_i\\big)$,  is the height and wingspan of player-$i$.\n",
    "We use the \"true\" superscript label here in anticipation of the fact that we will need to distinguish these measured wingspans from our model's predicted wingspans.\n",
    "Supposing we have some values picked out for our model's parameters, $m$ and $b$, then our model's predicted wingspan for player-$i$ is\n",
    "\n",
    "\\begin{equation}\n",
    "y^{\\mathrm{(pred)}}_i = F(m, b; x_i) = m x_i + b\n",
    "\\end{equation}\n",
    "\n",
    "Our goal is to find appropriate values for our model's parameters, $m$ and $b$, such that $F(m, b; x)$ will make reliable predictions about the wingspans of players whose measurements are not in our dataset.\n",
    "The way that we will measure the quality of our model's predictions is via the loss function (a.k.a objective function) that computes the mean squared-error (a.k.a squared residuals) of our predictions compared to our recorded data.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}_{\\mathrm{MSE}} = \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y^{\\mathrm{(true)}}_n - y^{\\mathrm{(pred)}}_n\\big)^2}\n",
    "\\end{equation}\n",
    "\n",
    "**To find the values of** $m$ **and** $b$ **that minimize** $\\mathscr{L}_{\\mathrm{MSE}}$ **is to produce the best-fit linear model - the one that minimizes empirical risk - for this dataset.**\n",
    "We will denote these optimal model parameter values as $m^*$ and $b^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e40e1b",
   "metadata": {},
   "source": [
    "Referring to the [previous section](https://rsokl.github.io/CogWeb/Video/Linear_Regression.html#Linear-Least-Squares:-A-Closed-Form-Solution), complete the following function that computes values of $m^*$ and $b^*$ directly, given our dataset.\n",
    "Note that we were already asked to do this in the reading comprehension question \"Ordinary Least Squares in Python\"; thus if you need guidance here, you can refer to the solution at the bottom of that page.\n",
    "\n",
    "We will be making use of this analytical solution so that we can precisely measure how far our approximate solution is away from the ideal; keep in mind that for most problems we will not have access to an exact solution like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_least_squares(x, y):\n",
    "    \"\"\"\n",
    "    Computes the slope and y-intercept for the line that minimizes\n",
    "    the sum of squared residuals of mx + b and y, for the observed data\n",
    "    (x, y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray, shape-(N,)\n",
    "        The independent data. At least two distinct pieces of data\n",
    "        are required.\n",
    "\n",
    "    y : numpy.ndarray, shape-(N,)\n",
    "        The dependent data in correspondence with ``x``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (m, b) : Tuple[float, float]\n",
    "        The optimal values for the slope and y-intercept\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97643159",
   "metadata": {},
   "source": [
    "Using this function, compute the parameters, $m^*$ and $b^*$, that minimize the MSE of a linear model for our dataset, where $x$ corresponds to \"height\" (without shoes) and $y$ corresponds to wingspan.\n",
    "We will want to access the underlying NumPy arrays from the xarray data so that you can work with the \"raw data\" conveniently here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute m* and b* that best fits the wingspan vs height (no shoes) data\n",
    "\n",
    "# Access the underlying numpy arrays\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Compute the ideal parameter values\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc256d",
   "metadata": {},
   "source": [
    "Next, plot the data for this problem along with the resulting fitted linear model.\n",
    "Use `ax.scatter` to plot the original data and `ax.plot` to draw the model line; \n",
    "you will want to specify a distinct color for your linear model.\n",
    "Label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wingspan vs height (no shoes) as a scatter plot\n",
    "# Plot the line: m* x + b*\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a601b",
   "metadata": {},
   "source": [
    "The following function creates a surface plot of $\\mathscr{L}_{\\mathrm{MSE}}(m, b)$ over a range of $(m, b)$ values, given a user-specified data set.\n",
    "Take a moment to read its docstring and then run this cell to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba884db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to modify this function at all. Just read\n",
    "# through it and run this cell \n",
    "\n",
    "def graph_linear_regression_mse(\n",
    "    x,\n",
    "    y,\n",
    "    trajectory=None,\n",
    "    m_scale=10,\n",
    "    b_scale=10,\n",
    "    sample_density=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the data `x, y`, plots the MSE surface on the space of possible\n",
    "    slope and y-intercept values for a linear regression model.\n",
    "\n",
    "    The plot is automatically centered at the optimal parameter values (m*, b*); this\n",
    "    point is demarcated with a black dot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape-(N,)\n",
    "        The x data from your dataset\n",
    "\n",
    "    y : np.ndarray, shape-(N,)\n",
    "        The y data from your dataset\n",
    "\n",
    "    trajectory : Optional[np.ndarray], shape-(T, 2) or shape-(N, T, 2)\n",
    "        One or more length-T sequence of (slope, intercept) points to superimpose over the surface.\n",
    "        This can be used to display a \"trajectory\" of parameter values.\n",
    "\n",
    "    m_scale : int, optional (default=10)\n",
    "        The size of the range of slopes that are plotted in each direction\n",
    "\n",
    "    b_scale : int, optional (default=10)\n",
    "        The size of the range of y-intercepts that are plotted in each direction\n",
    "\n",
    "    sample_density : int, optional (default=500)\n",
    "        The number of samples to calculate along each axis. Decreasing this speeds\n",
    "        up the plot at the cost of visual quality.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Figure, Axis]\n",
    "        Returns the matplotlib figure and axis that was created so\n",
    "        that the plot can be further manipulated or saved. \n",
    "    \"\"\"\n",
    "\n",
    "    def mse(x, y, m, b):\n",
    "        \"\"\"Computes the mean squared-error (MSE)\"\"\"\n",
    "        m = np.atleast_1d(m)\n",
    "        b = np.atleast_1d(b)\n",
    "        return ((x * m[None] + b[None] - y) ** 2).mean(axis=1)\n",
    "\n",
    "    # find least squares solution\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    m_opt, b_opt = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    l_opt = mse(x, y, m_opt, b_opt)\n",
    "\n",
    "    center_m = m_opt\n",
    "    center_b = b_opt\n",
    "\n",
    "    # Creates the plot figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection=\"3d\")\n",
    "\n",
    "    # Plot the local minimum of the MSE surface as a black dot\n",
    "    ax.plot(\n",
    "        [m_opt],\n",
    "        [b_opt],\n",
    "        l_opt,\n",
    "        c=\"black\",\n",
    "        marker=\"o\",\n",
    "        zorder=3,\n",
    "        markersize=7,\n",
    "    )\n",
    "\n",
    "    # Define quadratic surface of MSE landscape over m and b\n",
    "    m_series = np.linspace(center_m - m_scale, center_m + m_scale, sample_density)\n",
    "    b_series = np.linspace(\n",
    "        center_b - b_scale, center_b + b_scale, sample_density\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    Z = (b_series + x.reshape(-1, 1, 1) * m_series) - y.reshape(-1, 1, 1)\n",
    "\n",
    "    Z = np.mean(Z ** 2, axis=0)\n",
    "\n",
    "    # make surface plot\n",
    "    m_series, b_series = np.meshgrid(m_series, b_series)\n",
    "    ax.set_xlabel(\"Slope: m\")\n",
    "    ax.set_ylabel(\"Intercept: b\")\n",
    "    ax.set_zlabel(\"MSE Loss\")\n",
    "    ax.ticklabel_format(style=\"sci\", scilimits=(-1, 2))\n",
    "    ax.dist = 11\n",
    "    surf = ax.plot_surface(m_series, b_series, Z, cmap=plt.get_cmap(\"GnBu\"))\n",
    "\n",
    "    # Graphs one or more trajectories on the loss londscape\n",
    "    if trajectory is not None:\n",
    "        trajectories = np.atleast_2d(trajectory)\n",
    "        if trajectories.ndim == 2:\n",
    "            trajectories = trajectories[np.newaxis]\n",
    "        for trajectory in trajectories:\n",
    "            m_values, b_values = trajectory.T\n",
    "            l_values = ((x * m_values[:, None] + b_values[:, None] - y) ** 2).mean(\n",
    "                axis=1\n",
    "            )\n",
    "            ax.plot(\n",
    "                m_values,\n",
    "                b_values,\n",
    "                l_values,\n",
    "                marker=\"*\",\n",
    "                zorder=3,\n",
    "                markersize=7,\n",
    "            )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7c059",
   "metadata": {},
   "source": [
    "Invoke `graph_linear_regression_mse` for our dataset.\n",
    "Provide the `trajectory` argument the list of the optimal slope and intercept values that you computed via `ordinary_least_squares`. I.e. set `trajectory=[m_opt, b_opt]`.\n",
    "Your solution will appear as a star on the surface plot.\n",
    "The correct solution will appear as a black dot; thus the two should coincide.\n",
    "Note that you can click-and-drag your cursor over the plot to rotate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548dfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear regression MSE surface with your solution point included\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0af08b",
   "metadata": {},
   "source": [
    "This surface is our so-called **loss landscape**.\n",
    "It represents the values of $\\mathscr{L}_{\\mathrm{MSE}}$ over a continuum of $(m, b)$ parameter values.\n",
    "That is, given our fixed dataset, this depicts how well (or poorly) the various linear models of differing slopes and y-intercepts would fit the data.\n",
    "The ideal model is the one whose parameter values reside at the bottom of this loss-landscape.\n",
    "Typically we will use gradient descent to search for these ideal values that minimize $\\mathscr{L}$; as such, that is what we will be doing in the rest of this notebook.\n",
    "\n",
    "Before we proceed, note that our loss landscape looks much more like a taco shell (or a half pipe) than it does like a bowl, even though our equation for $\\mathscr{L}_{\\mathrm{MSE}}$ is quadratic in $m$ and $b$ (and 2D quadratic surfaces generally look like bowls).\n",
    "Even though the landscape looks flat along $b$, it is actually sloping upward away from the black dot; the slope is simply far more mild along the $b$ direction than it is along the $m$ direction.\n",
    "How might this affect the search for the minimum on this surface via gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd1ed4",
   "metadata": {},
   "source": [
    "## Estimating Optimal Model Parameters Using Gradient Descent\n",
    "\n",
    "In keeping with our [discussion of gradient-based learning](https://rsokl.github.io/CogWeb/Video/Gradient_Descent.html) we will arrive at near-optimal values for our model's parameters by utilizing gradient descent.\n",
    "\n",
    "We will start this process by crudely drawing (small) random values for $m$ and $b$; these will determine where we first reside on the loss landscape.\n",
    "Next we'll descend this loss landscape by iteratively updating the values for $m$ and $b$ with the gradient-based step\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}m_\\text{new} \\\\ b_\\text{new} \\end{bmatrix} &= \\begin{bmatrix} m_\\text{old} \\\\ b_\\text{old} \\end{bmatrix} - \\delta \\vec{\\nabla} \\mathscr{L}_{MSE} (m_{\\text{old}}, b_{\\text{old}})\\\\\n",
    "   &\\vdots\\\\\n",
    "   m_\\text{new} &= m_\\text{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}_{MSE}}{\\mathrm{d} m}\\big|_{m_{old}, b_{old}}\\\\\n",
    "   b_\\text{new} &= b_\\text{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}_{MSE}}{\\mathrm{d} b}\\big|_{m_{old}, b_{old}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\delta$ is the learning rate - a single, positive valued number that we have to pick.\n",
    "\n",
    "Keep in mind that we will be able to leverage automatic differentiation, via MyGrad, to compute $\\vec{\\nabla} \\mathscr{L}_{MSE} (m_{\\text{old}}, b_{\\text{old}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7495475",
   "metadata": {},
   "source": [
    "Complete the following function that performs a gradient-step on all of the supplied parameters.\n",
    "Note that this was a reading comprehension task - \"Writing a Generic Gradient-Update Function\" - in [a previous section](https://rsokl.github.io/CogWeb/Video/Automatic_Differentiation.html#Gradient-Descent-with-MyGrad).\n",
    "You can refer back to the solution at the bottom of the page for assistance with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function\n",
    "\n",
    "def gradient_step(tensors, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs gradient-step in-place on each of the provides tensors \n",
    "    according to the standard formulation of gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors : Union[Tensor, Iterable[Tensors]]\n",
    "        A single tensor, or an iterable of an arbitrary number of tensors.\n",
    "\n",
    "        If a `tensor.grad` is `None`for a specific tensor, the update on\n",
    "        that tensor is skipped.\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The gradient-steps performed by this function occur in-place on each tensor,\n",
    "    thus this function does not return anything\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072ece1",
   "metadata": {},
   "source": [
    "Now let's code up $\\mathscr{L}_{MSE}$ in MyGrad.\n",
    "\n",
    "We need to leverage MyGrad's auto-differentiation capabilities so that we can compute $\\mathscr{L}_{MSE} (m_{\\text{old}}, b_{\\text{old}})$, in order to perform gradient descent.\n",
    "Thus we must use mygrad's functions (instead of numpy) to compute the MSE of our model's predictions, in comparison to the \"truth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "def mean_squared_error_mygrad(y_pred, y_true):\n",
    "    \"\"\" Computers the mean-squared error for a collection of predictions\n",
    "    and corresponding true values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : mygrad.Tensor, shape-(N,)\n",
    "        A tensor of N predictions.\n",
    "\n",
    "    y_true : array_like, shape-(N,)\n",
    "        An array of N corresponding true values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mse : mygrad.Tensor, shape-()\n",
    "        A scalar-tensor containing the mean-squared error.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> pred = mg.Tensor([1., 2., 3.])\n",
    "    >>> true = mg.Tensor([1., 1., 3.])\n",
    "    >>> mean_squared_error_mygrad(pred, true)\n",
    "    Tensor(0.33333333)\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6a7f4",
   "metadata": {},
   "source": [
    "## Defining Our Linear Model\n",
    "\n",
    "We will now [create a class](https://www.pythonlikeyoumeanit.com/module_4.html) used to encapsulate the parameters and functionality of our linear model.\n",
    "Although this might seem to be excessive here, this will prime us for creating more sophisticated models (e.g. neural networks) later on.\n",
    "\n",
    "This class will be responsible for:\n",
    "\n",
    "- drawing random values to initialize `m` and `b`\n",
    "- storing our model parameters `m` and `b` and making them easily accessible\n",
    "- defining the so-called \"forward pass\" of our model: passing data to it and returning predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use `uniform` to draw random initial values for our model's parameters\n",
    "from mygrad.nnet.initializers import uniform\n",
    "\n",
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A linear model with parameters `self.m` and `self.b`\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Uses `mygrad.nnet.initializers.uniform` to draw tensor\n",
    "        values for both `self.m` and `self.b` from the uniform \n",
    "        distribution [-10, 10].\n",
    "        \n",
    "        Both parameters should be shape-(1,) tensors; the call:\n",
    "        \n",
    "           uniform(1, lower_bound=-10, upper_bound=10)\n",
    "        \n",
    "        will draw a shape-(1,) tensor from this distribution\n",
    "        \"\"\"\n",
    "        # Draw two values from the uniform distribution over [-10, 10]\n",
    "        # Assign one value to `self.m` and assign the other to `self.b`\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "    def __init__(self, m=None, b=None):\n",
    "        \"\"\" Accepts initial values for m and b. If either are not\n",
    "        specified, uses `self.initialize_params()` to draw them\n",
    "        randomly\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m : Optional[mygrad.Tensor], shape-(1,)\n",
    "            The slope for the linear model. If `None`, a random\n",
    "            value is drawn.\n",
    "            \n",
    "        b : Optional[mygrad.Tensor], shape-(1,)\n",
    "            The y-intercept for the linear model. If `None`, a random\n",
    "            value is drawn.\n",
    "        \"\"\"\n",
    "        # Use `self.initialize_params()` to draw random values for\n",
    "        # `self.m` and `self.b`. \n",
    "        #\n",
    "        # If either parameter is provided as an input to this method, \n",
    "        # use that specified value to overwrite the randomly drawn value\n",
    "        #\n",
    "        # `self.m` and `self.b` must be defined by this method\n",
    "        \n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs: m * x + b\n",
    "        \n",
    "        This is known as a 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the linear model to\n",
    "        make a prediction based on the input `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(N,)\n",
    "            An array or tensor of N observations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(N,)\n",
    "            A corresponding tensor of N predictions based on\n",
    "            the linear model.\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a tuple of the tensors associated with the model's\n",
    "        parameters.\n",
    "        \n",
    "        This is accessed as an attribute, via `model.parameters`\n",
    "        *not* as a method (i.e. not as `model.parameters()`)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tuple]\n",
    "            A tuple containing all of the learnable parameters for our model.\n",
    "            \n",
    "            This should return a tuple containing the slope and y-intercept \n",
    "            associated with the model.\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        >>> model = LinearModel()\n",
    "        >>> model.parameters\n",
    "        (Tensor([-7.714269], dtype=float32), Tensor([-6.770146], dtype=float32))\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fc2eb",
   "metadata": {},
   "source": [
    "Try initializing your model, `model = LinearModel()` and check that:\n",
    "\n",
    "- `model.m` and `model.b` are both defined and are shape-(1,) mygrad tensors\n",
    "- `model.parameters` returns both tensors\n",
    "- Calling `model(1.)` returns a mygrad tensor corresponding to: m * x + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your model class is working as-expected\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9c92f",
   "metadata": {},
   "source": [
    "## \"Training\" Our Model\n",
    "\n",
    "We will now use gradient descent to optimize our model's parameter values based on our recorded data.\n",
    "In the parlance of modern machine learning, this process is typically described as us \"training\" our model.\n",
    "And to introduce more terminology, the pattern of machine learning that we are about to invoke is called **supervised learning**: it is the process of training (updating) our model based off of collected data where we have access to the desired predictions that we want our model to make. That is, for each recorded height that we are going to feed to our model, we have an associated true wingspan that we measured and that we want our model to predict.\n",
    "\n",
    "### Using Un-Normalized Data (This Won't Work Well)\n",
    "\n",
    "To start off, we will attempt to search for ideal model parameters by processing our raw data (we will find that this works only moderately well and that we ought to pre-process our data before using it to train our model).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5851654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines a convenience function for measuring the\n",
    "# distance between (m, b) and (m*, b*)\n",
    "#\n",
    "# Run this without modifying it\n",
    "true_params = np.array(ordinary_least_squares(height, wingspan))\n",
    "\n",
    "def dist_from_true(model_params, true_params) -> float:\n",
    "    \"\"\" Computes sqrt[(m - m*)^2 + (b - b*)^2]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params : Tuple[Tensor, Tensor]\n",
    "        m and b\n",
    "    \n",
    "    true_params : numpy.ndarray, shape-(2,)\n",
    "        m* and b*\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The L2 distance between the parameters\"\"\"\n",
    "    params = np.array([i.item() for i in model_params])\n",
    "    return np.sqrt(np.sum((true_params - params) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35712f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = draft_data.height_no_shoes.data  # our observed data: x [inches]\n",
    "wingspans = draft_data.wingspan.data       # our observed data: y [inches]\n",
    "\n",
    "# Initialize your linear model without specifying any parameters.\n",
    "# Assign it to the variable called `model`\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# `trajectory` is a list that will store the sequence of estimated \n",
    "# model parameters that we compute during gradient descent:\n",
    "#           [(m0, b0), (m1, b1), ... ]\n",
    "trajectory = []\n",
    "\n",
    "# This is the number of times we will process the observed data\n",
    "# and perform an update our model's parameters\n",
    "#\n",
    "# An \"epoch\" denotes our having processed the dataset in its\n",
    "# entirety once. Thus we will train our model by processing \n",
    "# our dataset in full`num_epoch` times.\n",
    "num_epochs = 10\n",
    "\n",
    "# The learning rate used for gradient descent. \n",
    "#\n",
    "# This is a value that we need to choose. The following is simply\n",
    "# an educated guess of a good learning rate; there is a whole art\n",
    "# to making educated guesses and well-informed choices for learning\n",
    "# rates, which we will discuss later. The key thing to note here is that\n",
    "# there was no principled reason behind our picking this value\n",
    "learning_rate = 1E-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e0457",
   "metadata": {},
   "source": [
    "The following cell initialized a plot using the [noggin library](https://github.com/rsokl/noggin).\n",
    "This library is capable of logging measurements taken from an experiment and plot them in real time.\n",
    "Here, we tell noggin that we want to track four measured \"metrics\"\n",
    "\n",
    "- `\"loss\"`: the value of $\\mathscr{L}_{MSE}$ for our current model parameters\n",
    "- `\"m:`: the current slope of our model\n",
    "- `\"b:`: the current y-intercept of our model\n",
    "- `\"dist_from_target:`: the Euclidean distance of our model's $(m, b)$ from the optimal $(m^*, b^*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adec3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot created here will update in real time as we run our experiment\n",
    "\n",
    "from noggin import create_plot\n",
    "\n",
    "# Four \"metrics\" will be tracked by the plotter\n",
    "plotter, fig, ax = create_plot([\"loss\", \"m\", \"b\", \"dist_from_target\"], ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell will be responsible for training our model\n",
    "\n",
    "# Fill out the following code blocks and then run this cell.\n",
    "# View the noggin plot above to see the recorded metrics\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    # Perform a \"forward pass\" with your model on the *full* set of heights\n",
    "    # I.e. feed the model all N heights to produce N corresponding predictions\n",
    "    # using your lineary model\n",
    "    # STUDENT CODE HERE\n",
    "    \n",
    "    # Compute the mean-squared error of your model's predictions, compared\n",
    "    # to the true wingspans. Assign this value to the variable `loss`\n",
    "    # STUDENT CODE HERE\n",
    "    \n",
    "    # Use mygrad's auto-differentiation abilities to compute the derivatives\n",
    "    # of this error (a.k.a the loss) with respect to your model's parameters.\n",
    "    # I.e. invoke \"back-propagation\" from the computed loss.\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # This feeds our four measured metrics to noggin to be logged and plotted\n",
    "    # You don't need to change this\n",
    "    plotter.set_train_batch(\n",
    "        dict(\n",
    "            loss=loss,\n",
    "            m=model.m.item(),\n",
    "            b=model.b.item(),\n",
    "            dist_from_target=dist_from_true(model.parameters, true_params),\n",
    "        ),\n",
    "        batch_size=len(y_pred),\n",
    "    )\n",
    "    \n",
    "    # Appends the current model params to the \"trajectory\" list\n",
    "    trajectory.append((model.m.item(), model.b.item()))\n",
    "    \n",
    "    # Perform a single update to your model's parameters using\n",
    "    # gradient descent  (recall that you defined `gradient_step` earlier)\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "# This ensures that noggin plots any lingering measurements\n",
    "plotter.plot()\n",
    "\n",
    "# When you run this cell, the noggin plot above will update in real time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e31492",
   "metadata": {},
   "source": [
    "What does the graph of `dist_from_target` tell us about our approximate solution?\n",
    "Let's print out the last value of this metric associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the distance between our approximate solution and the exact one\n",
    "train_metrics = plotter.to_xarray(\"train\").batch\n",
    "train_metrics.dist_from_target[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca72fcb",
   "metadata": {},
   "source": [
    "Let's visualize the \"trajectory\" of our model's parameter values ‚Äì depicting how they evolved throughout \"training\".\n",
    "Does your the terminus of the trajectory end near the optimal solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_linear_regression_mse(height, wingspan, trajectory=trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85924b5",
   "metadata": {},
   "source": [
    "Let's see how well our learned model matches our data.\n",
    "The following plot will compare your learned model against the ideal model, derived from the closed-form solution to the least-squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "m, b =  ordinary_least_squares(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "ax.plot(x, model.m * x + model.b, c=\"orange\", label=\"Learned Fit\")\n",
    "ax.plot(x, m * x + b, c=\"red\", ls= \"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67887c19",
   "metadata": {},
   "source": [
    "You might see that the learned model does not match the least-squares solutions so closely (if it does match, then you got lucky! Try training the model again).\n",
    "It may match the model near the center of the data, but extrapolating outward would reveal discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b777fb8",
   "metadata": {},
   "source": [
    "Let's see just how varied our models will be.\n",
    "The following will train an \"ensemble\" of linear models in identical fashions - but with different randomly-drawn parameters.\n",
    "It will then plot the trajectory associated with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of linear models on our data\n",
    "# and plot their trajectories.\n",
    "#\n",
    "# Run this cell\n",
    "\n",
    "height = draft_data.height_no_shoes.data\n",
    "wingspan = draft_data.wingspan.data\n",
    "\n",
    "num_models = 10\n",
    "trajectories = [[] for i in range(num_models)]\n",
    "models = [LinearModel() for i in range(num_models)]\n",
    "\n",
    "num_epochs = 10\n",
    "step_size = 1e-4\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for model_id, model in enumerate(models):\n",
    "        y_pred = model(height)\n",
    "        loss = mean_squared_error_mygrad(y_pred, wingspan)\n",
    "        loss.backward()\n",
    "\n",
    "        trajectories[model_id].append((model.m.item(), model.b.item()))\n",
    "        gradient_step(model.parameters, learning_rate=learning_rate)\n",
    "\n",
    "trajectories = np.array(trajectories)\n",
    "graph_linear_regression_mse(height, wingspan, trajectory=trajectories)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "m, b = ordinary_least_squares(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "for n, model in enumerate(models):\n",
    "    ax.plot(x, model.m * x + model.b, alpha=0.5)\n",
    "ax.plot(x, m * x + b, c=\"red\", ls=\"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d6365",
   "metadata": {},
   "source": [
    "What is causing this stagnation in our optimization procedure?\n",
    "What is it about the shape of the \"landscape\" of $\\mathscr{L}_{MSE}$ that appears to keep our model from learning parameters that fit more closely to $(m^*, b^*)$?\n",
    "Consider these questions in light of the fact that we use the same learning rate for updating both $m$ and $b$.\n",
    "\n",
    "If you are working with others, discuss this with a neighbor and note your theories here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af648ea",
   "metadata": {},
   "source": [
    "*SOLUTION HERE*    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c9975",
   "metadata": {},
   "source": [
    "### Using Normalized Data\n",
    "\n",
    "The intense sensitivity of $\\mathscr{L}_{MSE}$ to changes in $m$ in comparison to changes in $b$ occurs because our data is centered far from the origin $(x=0, y=0)$.\n",
    "Thus, minute adjustments to $m$ cause a dramatic change to the predictions produced by our model near $(x=80, y=85)$, whereas changes to $b$ that are comparable in magnitude have a much less significant impact on the prediction quality.\n",
    "This is why $\\mathscr{L}_{MSE}$ looks like a flat valley along the $b$ axis compared to its steep slopes along the $m$ axis.\n",
    "Take sometime to reflect on this and test this statement if it isn't making sense at first.\n",
    "\n",
    "To remedy this we will want to **normalize our data** so that the normalized height and wingspan values both have a mean of $0$ and a standard deviation of $1$.\n",
    "Using this normalized data will help to produce a loss landscape that features comparable curvatures along the $m$ and $b$ directions.\n",
    "\n",
    "See that the following function will normalize an array of data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"Return `x_normed` such that x_normed.mean() is 0 \n",
    "    and x_normed.std() is 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like, shape-(N,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normed_x : array_like, shape-(N,)\n",
    "        The normalized data\"\"\"\n",
    "    return (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2103e4c",
   "metadata": {},
   "source": [
    "Lets normalize our height and wingspan data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_height = norm(height)\n",
    "normed_wingspan = norm(wingspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e00cf8",
   "metadata": {},
   "source": [
    "Compute the mean and standard deviation of `normed_height` and `normed_wingspan` and explicitly confirm that they have the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ef64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*SOLUTION HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794e7da",
   "metadata": {},
   "source": [
    "Let's plot this normalized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a84148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(normed_height, normed_wingspan)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d785f",
   "metadata": {},
   "source": [
    "Note that the scales of the numbers on the x and y axes have changed: now our data is centered on $(0, 0)$ and most of the values fall within $[-1, 1]$. \n",
    "That being said, the actual distribution of the data points relative to one another is entirely unchanged!\n",
    "That is, we have not in any way manipulated the patterns or relationships between height and wingspan that was encoded in the raw data. \n",
    "\n",
    "Let's try \"training\" our model again, but this time we will use our normalized data.\n",
    "Note how `dist_from_target` evolved here versus before - it should be very close to $0$ by then end of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter, fig, ax = create_plot([\"loss\", \"m\", \"b\", \"dist_from_target\"], ncols=2, last_n_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02979444",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = []\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# note that we are training for many more epochs\n",
    "# and with a much larger learning rate\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-1\n",
    "\n",
    "true_params_normed = np.array(ordinary_least_squares(normed_height, normed_wingspan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3460bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your earlier training-loop code here, but replace:\n",
    "# height -> normed_height\n",
    "# wingspan -> normed_wingspan\n",
    "# true_params -> true_params_normed\n",
    "\n",
    "# for n in range(num_epochs):\n",
    "#     ...\n",
    "#\n",
    "# STUDENT CODE HERE\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e036b8",
   "metadata": {},
   "source": [
    "How close is our approximate solution to the exact one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distance between our approximate solution and the exact one\n",
    "train_metrics = plotter.to_xarray(\"train\").batch\n",
    "train_metrics.dist_from_target[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad99012",
   "metadata": {},
   "source": [
    "Let's visualize the landscape for $\\mathscr{L}(m,b; (\\hat{x}_n, \\hat{y}_n)_{n=0}^{N-1})$ where $(\\hat{x}_n, \\hat{y}_n)_{n=0}^{N-1}$ represents our normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this\n",
    "\n",
    "graph_linear_regression_mse(normed_height, normed_wingspan, trajectory=trajectory)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(normed_height, normed_wingspan)\n",
    "m, b =  ordinary_least_squares(normed_height, normed_wingspan)\n",
    "\n",
    "x = np.linspace(normed_height.min(), normed_height.max(), 1000)\n",
    "\n",
    "ax.plot(x, model.m * x + model.b, c=\"orange\", label=\"Learned Fit\", lw=\"4\")\n",
    "ax.plot(x, m * x + b, c=\"red\", ls= \"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Normed Height\")\n",
    "ax.set_ylabel(\"Normed Wingspan\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace64135",
   "metadata": {},
   "source": [
    "See that the landscape no longer looks so flat ‚Äì adjusting $b$ and $m$ have comparable impacts on the quality of our model's predictions.\n",
    "Thus gradient descent will be much more effective at guiding our model's parameters towards $(m^*, b^*)$.\n",
    "Accordingly, our learned model now arrives at parameter values  that are very close to $(m^*, b^*)$\n",
    "This is all thanks to our having normalized our data before training on it.\n",
    "\n",
    "To see how much more reliable this training regimen is, let's train an ensemble of models, each with different initial parameters, and see that they all arrive very close to the same terminus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training an ensemble of models on normalized data\n",
    "\n",
    "num_models = 10\n",
    "trajectories = [[] for i in range(num_models)]\n",
    "models = [LinearModel() for i in range(num_models)]\n",
    "\n",
    "num_epochs = 100\n",
    "step_size = 1E-1\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for model_id, model in enumerate(models):\n",
    "        y_pred = model(normed_height)\n",
    "        loss = mean_squared_error_mygrad(y_pred, normed_wingspan)\n",
    "        loss.backward()\n",
    "\n",
    "        trajectories[model_id].append((model.m.item(), model.b.item()))\n",
    "        gradient_step(model.parameters, learning_rate=learning_rate)\n",
    "\n",
    "trajectories = np.array(trajectories)\n",
    "\n",
    "fig, ax = graph_linear_regression_mse(normed_height, normed_wingspan, trajectory=trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a1430",
   "metadata": {},
   "source": [
    "### Re-Scaling Our Model's Predictions\n",
    "\n",
    "Although we see that our model learns well on the normalized data, note that we no longer can simply feed a height (measured in inches) to our model and get a wingspan predicted in inches ‚Äì our model \"expects\" normalized data, and it's learned parameters will produce predicted wingspans on this \"normalized\" scale.\n",
    "I.e. it was trained to fit:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = m \\hat{x} + b\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "\\hat{x} &= \\frac{x - \\bar{x}}{\\mathrm{Std}[x]}\\\\\n",
    "\\hat{y} &= \\frac{y - \\bar{y}}{\\mathrm{Std}[y]}\n",
    "\\end{align}\n",
    "\n",
    "$\\bar{x}$ is the mean height of our observed data; $\\mathrm{Std}[x]$ is the corresponding standard deviation.\n",
    "$\\bar{y}$ is the mean height of our observed data; $\\mathrm{Std}[y]$ is the corresponding standard deviation.\n",
    "\n",
    "Given this, complete the following function that will permit us to pass \"raw\" heights to our model and for us to get \"raw\" wingspan predictions back.\n",
    "\n",
    "Hint: Take the above equation that transforms $y$ into $\\hat{y}$ and rewrite it so that $\\hat{y}$ is transformed to $y$.\n",
    "The output of our model, trained on normalized data, represents $\\hat{y}^{\\mathrm{(pred)}}$ and we need to transform it into $y^{\\mathrm{(pred)}}$ so that the prediction represents a wingspan in inches - as expressed in the original (raw) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3412fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_predictions(\n",
    "    model,\n",
    "    new_x,\n",
    "    height_mean=height.mean(),\n",
    "    height_std=height.std(),\n",
    "    wingspan_mean=wingspan.mean(),\n",
    "    wingspan_std=wingspan.std(),\n",
    "):\n",
    "    \"\"\" Given one or more input heights, measured in inches, uses the provided linear\n",
    "    model that was trainined on normalized data, to return the predicted wingspan in inches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Callable[[array_like], Tensor]\n",
    "        The linear model trained on normalized data\n",
    "    \n",
    "    new_x : array_like, shape-(N,)\n",
    "        N observed height values, measured in inches\n",
    "\n",
    "    height_mean : float\n",
    "        The mean of the height training data [inches]\n",
    "    \n",
    "    height_std : float\n",
    "        The std-dev of the height training data  [inches]\n",
    "        \n",
    "    wingspan_mean : float\n",
    "        The mean of the wingspan training data  [inches]\n",
    "    \n",
    "    wingspan_std : float\n",
    "        The std-dev of the wingspan training data  [inches]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(N,)\n",
    "        The N predicted wingspans, in inches, produced by the model\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Call `.data` on your model's output so that it produces a numpy array\n",
    "    and not a mygrad tensor.\n",
    "    \"\"\"\n",
    "    # First transform x into ùë•ÃÇ \n",
    "    # Then pass ùë•ÃÇ  into your model, the ourput represents ùë¶ÃÇ\n",
    "    # Finally, transform ùë¶ÃÇ into y, and return this numpy array\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff4a6c",
   "metadata": {},
   "source": [
    "Finally, let's plot predictions from our learned model, but by using `processed_predictions` to normalize the input data and \"rescale\" the resulting predictions to produce predicted wingspans on the desired scale (i.e. in inches).\n",
    "We should see that our learned model matches the ideal linear fit very closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8eae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "# Produce the processed predictions of your model, given the input `x`,\n",
    "# and assign the output to the variable `y`\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "\n",
    "ax.plot(x, y, color=\"orange\", lw=4, label=\"Learned Model\")\n",
    "\n",
    "\n",
    "m, b = ordinary_least_squares(height, wingspan,)\n",
    "ax.plot(x, m * x + b, c=\"red\", label=\"Ideal Fit\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916478a1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This exercise notebook provides the glue that connects the essential concepts that we have learned about thus far in our journey towards understanding machine learning using neural networks (i.e. deep learning).\n",
    "Namely, we:\n",
    "\n",
    "- Defined a mathematical model designed to transform observed data into useful predictions.\n",
    "  - In this case the model was a simple linear model, but we could easily generalize it to more complicated mathematical forms.\n",
    "  - It was natural for us to represent our model in terms of a [Python class](https://www.pythonlikeyoumeanit.com/module_4.html), since this allowed us to keep track of our model's parameters, our initialization scheme for the parameter values, and the code for performing a \"forward pass\" of our model on input data, all in one place.\n",
    "- Utilized automatic differentiation by using MyGrad's tensors and mathematical operations to store our model's parameters and to perform all of the mathematics associated with evaluating the model and the loss function for our problem. \n",
    "   - This gave us easy access to the gradient of the loss function with respect to our model's parameters. \n",
    "- Searched for optimal model parameter values - ones that minimize our loss function - by using gradient descent.\n",
    "   - We were introduced to the term \"epoch\" as an indicator that we had processed our dataset in its entirety.\n",
    "   - The selection of the learning rate was not informed by gradient descent or any obvious mathematics; we basically just made a guess at an appropriate value here (more on this later).\n",
    "   - This style of updating a mathematical model by using data containing the desired (or \"true\") predictions is known as \"supervised learning\".\n",
    "- Saw that the shape of our loss landscape had an impact on the efficacy of the gradient descent process, and, furthermore, that normalizing our data (to have a mean of $0$ and standard deviation of $1$) could help reshape this loss landscape to improve the model optimization process.\n",
    "   - This dynamic was rooted in the fact that the scales of the numbers associated with our raw data were such that making a small adjustment to $m$ made a much bigger impact on the quality of our model's predictions than did making a comparable adjustment to $b$.\n",
    "   - Normalizing our data helped to place $m$ and $b$ on more of an equal footing in terms of their influence on the model's predictions, and this led to healthier optimization performance, since we are using a single learning rate across all of the model's parameters.\n",
    "   - This parameter-scale balancing act will prove to be important for other, more sophisticated mathematical models as well, and data normalization will regularly be leveraged to help with this.\n",
    "\n",
    "It is recommended that you revisit and revise this notebook regularly to keep the lessons learned here in hand.\n",
    "\n",
    "In practice, we will never have the luxury of glimpsing the full loss landscape associated with our model and dataset as we did here.\n",
    "This is because our models will almost inevitable contain too many parameters to permit a plot of a 3D surface.\n",
    "So we wont have the benefit of qualitatively inspecting the trajectory of our gradient-based descent down the loss's surface, nor will we be able to easily glean the features of the surface's shape that prove difficult to traverse.\n",
    "For this reason, it is important to thoroughly internalize the lessons learned from this simple problem and prepare ourselves to anticipate their manifestations in more complicated scenarios - where we will need to be much more savvy and clever to deal with them."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-week2]",
   "language": "python",
   "name": "conda-env-.conda-week2-py"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
