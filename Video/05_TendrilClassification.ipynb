{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3415bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "from mygrad.nnet.initializers.he_normal import he_normal\n",
    "\n",
    "from datasets import ToyData\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b98000",
   "metadata": {},
   "source": [
    "In this notebook, we will be training a two-layer neural network to solve a *classification* problem on a toy data set. We will generate a spiral-formation of 2D data points. This spiral will have grouped \"tendrils\", and we will want our neural network to classify *to which tendril a given point belongs*. \n",
    "\n",
    "Read the documentation for `ToyData`. Run the following cells to view the spiral data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06221c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the spiral dataset and its labels.\n",
    "num_tendril = 3\n",
    "data = ToyData(num_classes=num_tendril)\n",
    "\n",
    "# Convert the data and labels to a datatype suitable for an `oxen` model.\n",
    "xtrain, ytrain, xtest, ytest = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47988aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = data.plot_spiraldata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c38fea",
   "metadata": {},
   "source": [
    "View the contents of `xtrain` and `ytrain`. What do these arrays correspond to? See that `xtrain` stores the 2D data points in the spiral, and `ytrain` stores the Tendril-ID associated with that point. How many points are in our training data? How are the labels specified for this dataset? Which label corresponds to which tendril? Discuss with your neighbor.\n",
    "\n",
    "*SOLUTION HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b9d6f",
   "metadata": {},
   "source": [
    "## Our Model\n",
    "\n",
    "We will extend the universal function approximation function to make a *classification* prediction. That is, given a 2D point $\\vec{x}$, our model will product $\\vec{y}_{pred} = [y_0, y_1, y_2]$, where $y_i$ indicates how \"confident\" our model is that $\\vec{x}$ belongs to tendril-$i$. \n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{\\vec{v}_i\\}, \\{\\vec{w}_i\\}, \\{b_i\\}; \\vec{x}) = \\sum_{i=1}^{N} \\vec{v}_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i}) = \\vec{y}_{pred}\n",
    "\\end{equation}\n",
    "\n",
    "Notice here that $\\vec{v}_i$ is now a *vector*, whereas in the original universal function approximation theorem it was a scalar. This is in accordance with the fact that we now want to *predict* a vector, $\\vec{y}_{pred}$, instead of a single number $y_{pred}$.\n",
    "\n",
    "What should the dimensionality of each $\\vec{v}_i$ be? Discuss with a neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35756ece",
   "metadata": {},
   "source": [
    "Create a two-layer dense neural network that closely resembles that one that you constructed as the universal approximator for the 1D functions. This time, however, $\\vec{x}$ will be a 2D point instead of a single number. Thus a batch of our training data will have a shape $(M, 2)$ instead of $(M, 1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bb32a",
   "metadata": {},
   "source": [
    "Because we are classifying which of three tendrils a 2D point belongs to, we now **want our model to predict *three* numbers**, rather than one, as its prediction (that is, it will produce three number per 2D point in the training batch). These three numbers will be the three \"scores\" that our model predicts for a point: one for each tendril. If score-0 is the largest score, then our model is predicting that the 2D point belongs to tendril 0. And so on. Thus the final layer of our network will have the shape $(M, 3)$ rather than $(M, 1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824798e",
   "metadata": {},
   "source": [
    "## The \"Activation\" Function\n",
    "\n",
    "We will be using the so-called \"activation function\" known as a \"rectified linear unit\", or ReLU for short:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi_{\\text{relu}}(x) = \n",
    "\\begin{cases} \n",
    "      0, \\quad x < 0 \\\\\n",
    "      x, \\quad 0 \\leq x\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This is a very popular activation function in the world of deep learning. We will not have time to go into why here, but it is worthwhile to read about. `mygrad` has this function: `mygrad.nnet.activations.relu`.\n",
    "\n",
    "(The astute reader will note that ReLU does not satisfy the requirements on $\\varphi(x)$, as dictated by the universal function theorem. Which requirement does this violate?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d2768",
   "metadata": {},
   "source": [
    "Import the relu function from `mygrad`, and plot it on $x \\in [-3, 3]$. What does the derivative of this function look like? Plot it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d436af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5fd2c",
   "metadata": {},
   "source": [
    "### Initializing Our Model Parameters\n",
    "We will be using a intialization technique known as \"He-normal\" initialization (pronounced \"hey\"). Essentially we will draw all of our dense-layer parameters from a scaled normal distribution, where the distribution will scaled by an additional $\\frac{1}{\\sqrt{2N}}$, where $N$ is dictates that number of parameters among $\\{\\vec{v}_i\\}_{i=0}^{N-1}$, $\\{\\vec{w}_i\\}_{i=0}^{N-1}$, and $\\{b_i\\}_{i=0}^{N-1}$, respectively. This will aid us when we begin training neural networks with large numbers of neurons.\n",
    "\n",
    "Import this initialization function from `MyNN`: `from mygrad.nnet.initializers.he_normal import he_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44728e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_neurons, num_classes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons : int\n",
    "            The number of 'neurons', N, to be included in the model.\n",
    "        \n",
    "        num_classes : int\n",
    "            The number of distinct classes that you want your model to predict.\n",
    "        \"\"\"\n",
    "        # set self.N equal to `num_neurons\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # set self.num_classes equal to the number of distinct\n",
    "        # classes that you want your model to be able to predict\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Use `self.initialize_params()` to draw random values for\n",
    "        # `self.w`, `self.b`, and `self.v` \n",
    "        \n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs a so-called 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the linear model to\n",
    "        make a prediction based on `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(M, 2)\n",
    "            An array of M observations, each a 2D point.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(M, num_classes)\n",
    "            A corresponding tensor of M predictions based on\n",
    "            the form of the universal approximation theorem.\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Randomly initializes and sets values for  `self.w`,\n",
    "        `self.b`, and `self.v`.\n",
    "        \n",
    "        Uses `mygrad.nnet.initializers.normal to draw tensor\n",
    "        values w, v from the he-normal distribution, using a gain of 1.\n",
    "        \n",
    "        The b-values are all initialized to zero.\n",
    "        \n",
    "        self.w : shape-???  ... using he-normal (default params)\n",
    "        self.b : shape-???  ... as a tensor of zeros\n",
    "        self.v : shape-???  ... using he-normal (default params)\n",
    "        \n",
    "        where `N` is the number of neurons in the model.\n",
    "        \"\"\"\n",
    "        # assign `self.m` and `self.b` each a tensor value drawn from\n",
    "        # the appropriate distribution\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def load_parameters(self, w, b, v):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19f4e5",
   "metadata": {},
   "source": [
    "## Computing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156fe220",
   "metadata": {},
   "source": [
    "Because we are solving a classification problem rather than a regression problem, we can measure the accuracy of our predictions. Write an `accuracy` function which accepts our models predictive scores for a batch of data, shape-(M, 3), and the \"truth\" labels for that batch, shape-(M,). \n",
    "\n",
    "Thus, if score-0 for some point is the maximum score, and the label for that point is 0, then the prediction for that point is correct. \n",
    "\n",
    "The function should return the mean classification accuracy for that batch of predictions (a single number between 0 and 1). Write a simple test for your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        The scores for D classes, for a batch of M data points\n",
    "        \n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, D)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accuracy: the fraction of predictions that your model got correct,\n",
    "        which should be in [0, 1]\"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5f93d",
   "metadata": {},
   "source": [
    "## Our Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ba35e",
   "metadata": {},
   "source": [
    "### The softmax activation function\n",
    "\n",
    "We will be using the \"cross-entropy\" function for our loss. This loss function is derived from the field of information theory, and is designed to compare probability distributions. This means that we will want to convert the numbers of $\\vec{y}_{pred} = F(\\{\\vec{v}_i\\}, \\{\\vec{w}_i\\}, \\{b_i\\}; \\vec{x})$, into numbers that behave like probabilities. To do this, we will use the \"softmax\" function:\n",
    "\n",
    "Given an $m$-dimensional vector $\\vec{y}$, the softmax function returns a a vector, $\\vec{p}$ of the same dimensionality:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{softmax}(\\vec{y}) = \\vec{p}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "p_i = \\frac{e^{y_{i}}}{\\sum_{j=0}^{m-1}{e^{y_{j}}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f60a8c",
   "metadata": {},
   "source": [
    "Convince yourself that the elements of $\\vec{p}$ do indeed satisfy the basic requirements of being a probability distribution. I.e. :\n",
    "\n",
    "- $0 \\leq p_i \\leq 1$, for each $p_i$\n",
    "- $\\sum_{i=0}^{m-1}{p_i} = 1$\n",
    "\n",
    "where $m$ is the number of classes in our classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e62a67",
   "metadata": {},
   "source": [
    "### The cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11ad6c",
   "metadata": {},
   "source": [
    "So, armed with the softmax function, we can convert our classification scores, $\\vec{y}$, to classification probabilities, $\\vec{p}$ (or at the very least, numbers that *act* like probabilities. This opens the door for us to utilize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "Given our prediction probabilities for the $m$ classes in our problem, $\\vec{p}$, we have the associated \"true\" probabilities $\\vec{t}$ (since we are solving a supervised learning problem). E.g., \n",
    "\n",
    "- if our point $\\vec{x}$ resides in tendril-0, then $\\vec{t} = [1., 0., 0.]$\n",
    "- if our point $\\vec{x}$ resides in tendril-1, then $\\vec{t} = [0., 1., 0.]$\n",
    "- if our point $\\vec{x}$ resides in tendril-2, then $\\vec{t} = [0., 0., 1.]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b71926",
   "metadata": {},
   "source": [
    "In terms of our predictions, $\\vec{p}$, and our truth-values, $\\vec{t}$, the cross-entropy loss is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}(\\vec{p}, \\vec{t}) = -\\sum_{i=0}^{m}{t_{i}\\log{p_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c248b40",
   "metadata": {},
   "source": [
    "This loss function measures *how different two probability distributions, $\\vec{p}$ and $\\vec{t}$ are*. The loss gets larger as the two probability distributions become more disparate, and the loss is a minimum (0) when the two distributions are identical.\n",
    "\n",
    "In the case we only have two classes, such that $\\vec{p}=\\begin{bmatrix}p, & 1-p\\end{bmatrix}$ and $\\vec{t}=\\begin{bmatrix}t, & 1-t\\end{bmatrix}$, we can actually visualize our loss function as below:\n",
    "\n",
    "![loss](pics/loss.png)\n",
    "\n",
    "---\n",
    "\n",
    "Is this what you expect to see from the loss function? Discuss with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cfd994",
   "metadata": {},
   "source": [
    "## The softmax-crossentropy function\n",
    "\n",
    "Because it is very common to perform the softmax on the outputs of your model, to \"convert them to probabilities\", and then pass those probabilities to a cross-entropy function, it is more efficient to have a function that does both of these steps. This is what `mygrad`'s [softmax_crossentropy](https://mygrad.readthedocs.io/en/latest/generated/mygrad.nnet.losses.softmax_crossentropy.html) function does. Take the time to read its documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facff377",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from mygrad.nnet.losses import softmax_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faabe85",
   "metadata": {},
   "source": [
    "## Defining your gradient descent and forward pass functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(params, learning_rate):\n",
    "    \"\"\" Update tensors according to vanilla gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Sequence[mygrad.Tensor]\n",
    "        A list/tuple of learnable parameters to be updated\n",
    "\n",
    "    learning_rate : float\n",
    "        The 'step size' to use during the descent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Each parameter in the list should be updated 'in-place'\"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74883a",
   "metadata": {},
   "source": [
    "Initialize your noggin plotter so that it will track two metrics: loss and accuracy\n",
    "\n",
    "```python\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n",
    "```\n",
    "\n",
    "Also, initialize your model parameters and batch-size. \n",
    "- Start off with a small number of neurons in your layer - try $N=3$. Increase number of parameters in your model to improve the quality of your result. You can use the visualization that we provide at the end of this notebook to get a qualitative feel for your notebook\n",
    "- A batch-size of 50 is fine, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will recreate your model, re-initializing all of its parameters\n",
    "# Thus you must re-run this cell if you want to train your model from scratch again.\n",
    "\n",
    "# - Create the noggin figure using the code snippet above\n",
    "# - Set `batch_size = 50`: the number of predictions that we will make in each training step\n",
    "# - Create your model\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9f64b",
   "metadata": {},
   "source": [
    "Referring to the code that you used to train your universal function approximator, write code to train your model on the spiral dataset for a specified number of epochs. Remember to shuffle your training data before you form batches out of it. Also, remember to use the the `softmax_crossentropy` loss.\n",
    "\n",
    "Try training your model for 1000 epochs. A learning rate $\\approx 0.1$ is a sensible starting point. Watch the loss and accuracy curves evolve as your model trains.\n",
    "\n",
    "Below, you will be able to visualize the \"decision boundaries\" that your neural network learned. Try adjusting the number of neurons in your model, the number of epochs trained, the batch size, and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the loss function from mygrad, as directed above\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# specify your learning rate\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "for epoch_cnt in range(1000):\n",
    "\n",
    "    # `idxs` will store shuffled indices, used to randomize the batches for\n",
    "    # each epoch\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(0, len(xtrain) // batch_size):\n",
    "        \n",
    "        # get the batch-indices from `idxs` (refer to the universal function approx notebook)\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # index into `xtrain` and `ytrain` with these indices to produce\n",
    "        # the batch of data and the associated tendril-labels for this batch\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # make predictions with your model\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # Compute the loss associated with our predictions\n",
    "        # The loss should be a Tensor so that you can do back-prop\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # Trigger back-propagation through your computational graph,\n",
    "        # so that dL/dw, dL/db, and dL/dv are computed\n",
    "        loss.backward()  # <COGINST>\n",
    "\n",
    "        # Perform gradient descent\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Compute the accuracy of the predictions\n",
    "        # The accuracy should just be a floating point number\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        plotter.set_train_batch(\n",
    "            {\"loss\": loss.item(), \"accuracy\": acc}, batch_size=batch_size\n",
    "        )\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e844580",
   "metadata": {},
   "source": [
    "This cell will allow you to visualize the decision boundaries that your model learned. We must define a function whose only input is the data, and whose out output is the softmax (**not** softmax crossentropy) of your classification scores.\n",
    "\n",
    "For this to work, the parameters that you defined for your model must have the names `w`, `b`, `v`, as used below (ask for help if this isn't working for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c494f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    return softmax(model(x)).data\n",
    "\n",
    "fig, ax = data.visualize_model(dummy_function, entropy=False);"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python [conda env:week2]",
   "language": "python",
   "name": "conda-env-week2-py"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
