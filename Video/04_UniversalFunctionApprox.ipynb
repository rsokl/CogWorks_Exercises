{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525a409",
   "metadata": {},
   "source": [
    "# Training a Universal Function Approximator\n",
    "\n",
    "The *universal approximation theorem* states:\n",
    "\n",
    "Let $ \\varphi (\\cdot )$ be a nonconstant, bounded, and monotonically-increasing continuous function. Let $ I_m $ denote any compact subset of $ \\mathbb {R} ^{m} $. The space of continuous functions $I_m  \\mapsto \\mathbb {R}$ is denoted by $C(I_{m})$. \n",
    "\n",
    "Then, given any $\\varepsilon >0$ and any function $f\\in C(I_{m})$, there exist $N$ real constants $v_{i},b_{i}\\in \\mathbb {R}$ and real vectors $\\vec{w}_{i}\\in \\mathbb {R} ^{m}$, where $i=1,\\cdots ,N$, such that we may define:\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{v_i\\}, \\{\\vec{w}_i\\}, \\{b_i\\}; \\vec{x}) = \\sum_{i=1}^{N} v_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "as an approximate realization of a function $f(x)$, where $f$ is independent of $\\varphi$ ; that is,\n",
    "\n",
    "\\begin{equation}\n",
    "| F( \\vec{x} ) - f ( \\vec{x} ) | < \\varepsilon\n",
    "\\end{equation}\n",
    "\n",
    "for all $ x\\in I_{m}$. See that $\\vec{x} \\cdot \\vec{w}_{i}$ is the dot product between $\\vec{x}$ and $\\vec{w}_{i}$, which are vectors in an $m$-dimensional space. Each ${b_i}$ is a scalar and each ${v_i}$ is a scalar. In later work we will want to extend $v_i$ to be a vector, so here we will also treat it as a vector in a 1-dimensional space, to make our future transition seamless.\n",
    "\n",
    "This theorem was first proven in 1989, using the *sigmoid function* as $\\varphi$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fef90",
   "metadata": {},
   "source": [
    "## Our problem\n",
    "Here, we will try to find values for the parameters $N,v_{i},b_{i},w_{i}$  (where $i=1,\\cdots ,N$) such that $F(x)$ approximates \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\cos(x)\\\\\n",
    "x \\in [-2\\pi, 2\\pi]\n",
    "\\end{equation}\n",
    "\n",
    "Using the sigmoid function as $\\varphi$.\n",
    "\n",
    "Because $\\cos(x)$ maps $[-2\\pi, 2\\pi]  \\mapsto [-1, 1]$, $x$ and $w_{i}$ are scalars. In the future we will be working with high dimensional data, so we will want to treat $x$ and $w_{i}$ as vectors in a 1-dimensional space (i.e. length-1 tensors) here; this will make it trivial to adapt our code to higher dimensional data later on.\n",
    "\n",
    "We will search for optimal values of $v_{i},w_{i},b_{i}$ via *gradient descent*, using the obvious **loss function**:\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x) = | F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x ) - \\cos ( x ) |\n",
    "\\end{equation}\n",
    "\n",
    "The *number* of parameters to use, $N$, is a **hyper parameter**, which we must find through trial and error, or some other means. $N$ is not something we can determine via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ef924",
   "metadata": {},
   "source": [
    "### Plotting our \"activation function\"\n",
    "\n",
    "Import the `sigmoid` from `mygrad.nnet.activations`. Plot this function on the domain $[-10, 10]$. \n",
    "\n",
    "Is this a \"nonconstant, bounded, and monotonically-increasing continuous function\", as demanded for $\\varphi$ by the universal approximation theorem?\n",
    "\n",
    "What does the sigmoid function do to \"extreme\" values of $x$? What mechanism might this serve? Discuss with neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe926f0c",
   "metadata": {},
   "source": [
    "### Write a gradient-descent function\n",
    "Write a gradient descent function that accepts a tuple of tensors and a **learning rate** ($\\delta$). \n",
    "\n",
    "**For each tensor in a list/tuple**, update the tensor's *underlying numpy array* using to gradient descent. Skip the tensor if its gradient is `None`. Because you are modifying the data of these tensors in-place, this function need not return anything. Write a good docstring for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0440a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(tensors, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs gradient-step in-place on each of the provides tensors \n",
    "    according to the standard formulation of gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors : Union[Tensor, Iterable[Tensors]]\n",
    "        A single tensor, or an iterable of an arbitrary number of tensors.\n",
    "\n",
    "        If a `tensor.grad` is `None`for a specific tensor, the update on\n",
    "        that tensor is skipped.\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The gradient-steps performed by this function occur in-place on each tensor,\n",
    "    thus this function does not return anything\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66c52c",
   "metadata": {},
   "source": [
    "## Defining Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb4d3f",
   "metadata": {},
   "source": [
    "### Working with \"Batches\" of Data\n",
    "It is computationally inefficient to train our model by passing it one datum, $x$, at a time. Rather, we will want to pass in a **batch** of $M$ pieces of input data, $\\{x_{j}\\}_{j=0}^{M-1}$, and evaluate our model for each of these values independently.  That is, we will pass on a batch of $M$ pieces of data and produce $M$ corresponding predictions from our model. \n",
    "\n",
    "Each prediction is made only on its corresponding piece of input data. Thus, prediction $F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j )$ depends only on:\n",
    "\n",
    "- our model's parameters: $\\{v_i\\}, \\{w_i\\}, \\{b_i\\}$ \n",
    "- datum $x_j$\n",
    "\n",
    "it is **not** impacted by any of the other pieces of data in the batch. This is very important to keep in mind!\n",
    "\n",
    "We will make our $M$ predictions for the batch using vectorization and not for-loops. Thus `x` will be a shape-$(M, 1)$ numpy-array instead of a single number.\n",
    "\n",
    "Recall that\n",
    "\n",
    "\\begin{equation}\n",
    "x_j \\cdot w_{i}\n",
    "\\end{equation}\n",
    "\n",
    "can be evaluated all combinations of $j=1,\\cdots ,M$ and $i=1,\\cdots ,N$ via simple matrix multiplication between the shape-$(M, 1)$ `x` and the shape-$(1, N)$ `w`, producing a shape-$(M, N)$ output. And thus the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x_{j} \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "can be performed for all $j$ and $i$ via broadcasting:\n",
    "\n",
    "```python\n",
    "sigmoid(mg.matmul(x, w) + b) # matmul[(M,1) w/ (1, N)] + (N,) --> (M, N)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab6ed7",
   "metadata": {},
   "source": [
    "Multiplying each such term by $v_{i}$ and summing over $i$ is thus just another matrix multiplication:\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) = \\sum_{i=1}^{N} v_{i}\\varphi(x_{j} \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "can be performed for each `j` in the batch via:\n",
    "```python\n",
    "out1 = sigmoid(mg.matmul(x, w) + b)  # matmul[(M,1) w/ (1, N)] + (N,) --> (M, N)\n",
    "model_out = mg.matmul(out1, v)       # matmul[(M, N) w/ (N, 1)] --> (M, 1)\n",
    "```\n",
    "\n",
    "Thus `model_out` is a shape-$(M, 1)$ tensor that holds the prediction of our model, corresponding with each datum in our shape-(M, 1) batch. \n",
    "\n",
    "Define the `Model.__call__` method such that it accepts a batch of shape-(M, 1), and produces (M, 1) predictions. Include detailed comments about what the input and output shapes are of all the tensors in this so-called forward-pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def initialize_params(self, num_neurons: int):\n",
    "        \"\"\"\n",
    "        Randomly initializes and sets values for  `self.w`,\n",
    "        `self.b`, and `self.v`.\n",
    "        \n",
    "        Uses `mygrad.nnet.initializers.normal to draw tensor\n",
    "        values w, b, and v from a normal distribution with\n",
    "        0-mean and std-dev of 1.\n",
    "        \n",
    "        self.w : shape-(1, N)\n",
    "        self.b : shape-(N,)\n",
    "        self.v : shape-(N, 1)\n",
    "        \n",
    "        where `N` is the number of neurons in the model.\n",
    "        \"\"\"\n",
    "        # assign `self.w`, `self.b`, and `self.v` each a tensor value drawn from\n",
    "        # the appropriate distribution\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "    def __init__(self, num_neurons: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons : int\n",
    "            The number of 'neurons', N, to be included in the model.\n",
    "        \"\"\"\n",
    "        # set self.N equal to `num_neurons\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Use `self.initialize_params()` to draw random values for\n",
    "        # `self.w`, `self.b`, and `self.v` \n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs a so-called 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the model to\n",
    "        make a prediction based on `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(M, 1)\n",
    "            An array of M observations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(M, 1)\n",
    "            A corresponding tensor of M predictions based on\n",
    "            the form of the universal approximation theorem.\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def load_parameters(self, w, b, v):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a73934",
   "metadata": {},
   "source": [
    "### Writing a loss function\n",
    "\n",
    "For the problem at hand, given how the universal approximation theorem is posed, it is quite natural to choose our loss function to be:\n",
    "\n",
    "\\begin{equation}\n",
    "L(x) = | F( \\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x ) - \\cos ( x ) |\n",
    "\\end{equation}\n",
    "\n",
    "which is called the \"L1-loss\". Our loss grows linearly with the disagreement between our approximating function (a.k.a our \"predictions\") and the \"true\" function.\n",
    "\n",
    "Note, however, that we want to make predictions size-$M$ batches. Thus we will have:\n",
    "\n",
    "\\begin{equation}\n",
    "L(x_{j}) = | F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) - \\cos ( x_{j} ) |\n",
    "\\end{equation}\n",
    "\n",
    "for each $j=1,\\cdots ,M$. \n",
    "\n",
    "That being said, we want to ultimately have a *single scalar loss*. Let's choose to compute our total loss as the *average* over the $M$ values, $L(x_{j})$. (Note that this average-loss is called \"risk\" in machine learning and game theory literature).\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; \\{x_k\\} ) = \\frac{1}{M}\\sum_{j=1}^{M} | F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) - \\cos ( x_{j} ) |\n",
    "\\end{equation}\n",
    "\n",
    "Write the function `l1_loss`, which accepts the shape-$(M,1)$ batch of **predictions** from our model along with the shape-$(M, 1)$ **true** values (which we are hoping to approximate) and returns the average loss, $L$.\n",
    "\n",
    "Make sure that you are using functions from mygrad and not numpy, so that we can back-propagate through this loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(pred, true):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : mygrad.Tensor, shape=(M,)\n",
    "    true : mygrad.Tensor, shape=(M,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mygrad.Tensor, shape=()\n",
    "        The l1-loss averaged over the size-M batch of predictions\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29d0b1",
   "metadata": {},
   "source": [
    "## Preparing the training data\n",
    "You will create a numpy-array or *constant-Tensor* that samples $[-2\\pi, 2\\pi]$ 1,000 evenly-spaced points. Call this `train_data`. This should have the shape of (1000, 1). (You will do this lower down in the notebook).\n",
    "\n",
    "Why is it important that we use numpy-arrays or constant tensors? Why would it be inefficient to perform back-propagation if our training data were non-constant tensors? Discuss with your neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159174e5",
   "metadata": {},
   "source": [
    "### Training Our Approximating Function! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2b592",
   "metadata": {},
   "source": [
    "We will use the handy 'noggin' package to keep track of how our loss is evolving during training. The following code will create a plotter that will keep track of \"loss\", and will refresh itself every 2 seconds, during training:\n",
    "\n",
    "```python\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\"])\n",
    "ax.set_ylim(0, 1)\n",
    "```\n",
    "\n",
    "We will need to make take randomized \"batches\" of our training data, and use them to train our model. Each time we process all of the batches in our training data, we have completed an \"epoch\" of training.\n",
    "\n",
    "Below, we will use batches of size-25. Thus we will need to process $1000/25 = 40$ batches to complete an epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6d803",
   "metadata": {},
   "source": [
    "Here, we will set up our training data, initialize our model's parameters, set our batch size, and define the function that we are attempting to approximate. We will also create a plot that updates during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will recreate your model, re-initializing all of its parameters\n",
    "# Thus you must re-run this cell if you want to train your model from scratch again.\n",
    "\n",
    "# Create the noggin figure using the code snippet above\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Create the shape-(1000,1) training data\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Initialize your model; \n",
    "# start off with N=10 neurons\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Set `batch_size = 25`: the number of predictions that we will make in each training step\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Define the function `true_f`, which should just accept `x` and return `np.cos(x)`\n",
    "# (or any other function that you want to approximate later on)\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "\n",
    "# we will store our model's weights in this list every 10 epochs \n",
    "#so that we can assess what our model's predictions look like mid-training\n",
    "params = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this code will train your model; you can run it consecutively,\n",
    "# with different learning rates to continue training your model.\n",
    "\n",
    "# set your \"learning rate\": the scaling parameter in gradient descent\n",
    "# try a value of 0.01 to start\n",
    "#\n",
    "# If your loss function plateaus, you can reduce this by 10x and\n",
    "# resume training to further reduce the loss\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# We will train for 1000 epochs; you can change this if you'd like\n",
    "for epoch_cnt in range(1000):\n",
    "    # We need to create randomly-drawn batches of data...\n",
    "\n",
    "    # Produce a numpy array of integer indices (starting at 0) - one for\n",
    "    # each datum in your training\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # Use np.random.shuffle to shuffle these indices.\n",
    "    # Note that this functions will sort  the indices *in-place* - it does\n",
    "    # not return anything\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # Let's keep track of our model's progress. Every 10 epochs we'll\n",
    "    # record our model's weights so that we can visualize what its\n",
    "    # predictions look like as it was training.\n",
    "    if epoch_cnt % 10 == 0:\n",
    "        params.append([w.data.copy() for w in [model.w, model.b, model.v]])\n",
    "\n",
    "    for batch_cnt in range(0, len(train_data) // batch_size):\n",
    "\n",
    "        # Take a size-`batch_size` slice from the randomized indices that you created\n",
    "        # above. Each batch count should produce the subsequent, non-overlapping slice.\n",
    "        #\n",
    "        # Remember that the 'stop' point of the slice can exceed the end of the array -\n",
    "        # it will just produce a shorter slice. This means you don't need to worry about\n",
    "        # your batch-size dividing into your data evenly - your lase batch might just\n",
    "        # be a bit smaller than before.\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Use the resulting batch-indices to get the corrsponding batch\n",
    "        # of training data\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # compute the predictions for this batch: F(x)\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # compute the true (a.k.a desired) values for this batch: f(x)\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # compute the loss associated with our predictions\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # back-propagate through your computational graph through your loss\n",
    "        # this will compute: dL/dw, dL/db, dL/dv\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # execute your gradient descent function, passing all of your model's\n",
    "        # parameters (w, b, v), and the learning rate. This will update your\n",
    "        # model's parameters based on the loss that was computed\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # this will record the current loss, and will plot it\n",
    "        plotter.set_train_batch({\"loss\": loss.item()}, batch_size=batch_size)\n",
    "    plotter.set_train_epoch()\n",
    "\n",
    "# this will ensure you plotted the most recent data\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431311c8",
   "metadata": {},
   "source": [
    "Remember that we were just updating our model's parameters, $\\{v_i\\}, \\{w_i\\}, \\{b_i\\}$, using gradient descent so as to minimize our loss: L(x) = $| F( \\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x ) - \\cos ( x ) |$\n",
    "\n",
    "Thus we should expect to see that $F( \\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x ) \\approx \\cos ( x )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a3db9",
   "metadata": {},
   "source": [
    "To evaluate the quality of your model (i.e. your approximating function $F(x)$), plot $f(x)$ (the desired function) and $F(x)$ on the sample plot. Use `train_data` as your `x` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076afc43",
   "metadata": {},
   "source": [
    "Let's see what our model looked like *as it was training/learning*. Run the following cell to see the true function (plotted in blue) and our approximating function (plotted in orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "x = np.linspace(-4 * np.pi, 4 * np.pi, 1000)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, np.cos(x))\n",
    "ax.set_ylim(-2, 2)\n",
    "_model = Model(model.N)\n",
    "_model.load_parameters(*params[0])\n",
    "(im,) = ax.plot(x.squeeze(), _model(x[:, np.newaxis]).squeeze())\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    # ax.figure.canvas.draw()\n",
    "    _model.load_parameters(*params[frame])\n",
    "    im.set_data(x.squeeze(), _model(x[:, np.newaxis]).squeeze())\n",
    "    return (im,)\n",
    "\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=range(0, len(params)),\n",
    "    interval=20,\n",
    "    blit=True,\n",
    "    repeat=True,\n",
    "    repeat_delay=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e8ab7",
   "metadata": {},
   "source": [
    "Let's visualize the form learned by each of our \"neurons\", scaled by . That is, plot\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "for each $i=1, ..., N$ on $x \\in [-2\\pi, 2\\pi]$. Note that $v_i$ is *not* included here.\n",
    "\n",
    "In the following, `axes` is an array that stores two columns of matplotlib-axis objects such that `axes.size` matches the number of neurons in your model.\n",
    "Plot the form of neuron-$i$ using `ax = axes.flatten()[i]` and then `ax.plot(x, sigmoid(x * w_i + b_i))` for the appropriate `w_i` and `b_i`  each neuron.\n",
    "\n",
    "Use a for-loop. It might be useful to use `flatten()` on your model parameters.\n",
    "You might also consider using [zip](https://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/Itertools.html); but that is just a matter of convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=model.N // 2)\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi)\n",
    "\n",
    "\n",
    "# Using a for-loop, plot the output of each scaled neuron:\n",
    "#     v * sigmoid(w*x + b)\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525dd5f",
   "metadata": {},
   "source": [
    "Finally, let's include the scaling factors, \\{v_i\\}, each of which multiplies a respective neuron. That is, plot\n",
    "\n",
    "\\begin{equation}\n",
    "v_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "for each $i$ on $x \\in [-2\\pi, 2\\pi]$. \n",
    "\n",
    "**What will the result look like if you plot the sum of all of these curves? (Hint: look back to the form of the universal function approximation theorem**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf2196",
   "metadata": {},
   "source": [
    "Your response here:\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e44b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi)\n",
    "\n",
    "# plots the full model output as a thick dashed black curve\n",
    "ax.plot(\n",
    "    x,\n",
    "    model(x[:, np.newaxis]),\n",
    "    color=\"black\",\n",
    "    ls=\"--\",\n",
    "    lw=4,\n",
    "    label=\"full model output\",\n",
    ")\n",
    "\n",
    "# Add to the plot the scaled activation for each neuron: v Ïƒ(x * w + b)\n",
    "# Plot each of these using the same instance of `Axes`: `ax`.\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Visualizing the 'activity' of each of the model's scaled neurons\")\n",
    "ax.set_xlabel(r\"$x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb256c3",
   "metadata": {},
   "source": [
    "### Things to Try (be sure to preserve any good code you have before trying these things)\n",
    "- Once your loss curve (often called the \"learning curve\") plateaus, try reducing the learning rate and resume training. This will likely lower your loss further.\n",
    "- Once you have a good model, try plotting $f(x)$ and $F(x)$ *beyond* the domain that you trained it on. For example, try plotting them on $[-4\\pi, 4\\pi]$. What do you see? Is this reasonable? Discuss with a neighbor.\n",
    "  - Dig in a little deeper, plot each of the model's scaled neurons $v_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})$ on $[-4\\pi, 4\\pi]$. You should be able to visually see how the sigmoidal curves will sum outside of $[-2\\pi, 2\\pi]$\n",
    "- Try decreasing the the parameter-number in your model from $N=10$ down to $N=1$. Thus `w` will have the shape (1, 1) instead of (1, 10), etc. Train this model as best you can, and plot $F(x)$. What shape does this take? Can you explain why?\n",
    "- Using $N=10$, repeat your training but train on the domain $[2\\pi, 6\\pi]$. Are you able to get your model to train well? Why should shifting the domain have any affect if $f(x)$ is perfectly periodic. Consider what special properties our original domain, $[-2\\pi, 2\\pi]$ has. Consider, also, how we initialize our model's parameters. Discuss with your neighbor what you suspect might be the issue here. You can use `noggin` to plot the mean values of `w`, `v`, and `b` as you train. You can also plot the mean values of the gradients that are back-propagating through your model, with some minor modifications to your code. This is very interesting to visualize.\n",
    "- Fit a different $f(x)$ other than cosine. Do you need more parameters to approximate more complicated functions?\n",
    "- Try increasing $N$ to $N=1000$. You may need to try adjusting your learning rate during training, lowering it as you go. Does increasing $N$ make things better or worse in this instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting F(x) outside of its training domain\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d46001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each of the scaled neurons, v*sigmoid(w*x + b), on [-4 pi, 4 pi]\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model using N=1\n",
    "\n",
    "# INTERPRETATION: For N=1, F(x) = v * sigmoid(x*w + b), thus F(x) must\n",
    "# have the form of a sigmoid function (albeit a shallow one)\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a409e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing w_grad.mean()\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-new_week2]",
   "language": "python",
   "name": "conda-env-.conda-new_week2-py"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
