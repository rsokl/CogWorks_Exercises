{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a7585",
   "metadata": {},
   "source": [
    "# Simple RNN Cell in MyGrad\n",
    "\n",
    "In this notebook, we will implement a simple RNN model that can be used for sequence classification problems.\n",
    "We'll apply this RNN to the **classification problem of determining if a sequence of digits (0-9) is the concatentation of two identical halves.**\n",
    "\n",
    "For example:\n",
    "- `[1, 2, 3, 1, 2, 3]` -> contains identical halves\n",
    "- `[1, 9, 2, 1, 8, 3]` -> does not contain identical halves\n",
    "\n",
    "Our model will take a single sequence of data, $(\\vec{x}_t)_{t=1}^T$, stored in a shape $(T, C)$ array, where $T$ is the length of our sequence and $C$ is the embedding dimensionality of each entry in our sequence.\n",
    "The model will produce $K$ classification scores, assuming there are $K$ classes for the problem. \n",
    "\n",
    "In the context of word-embeddings, if each word in our vocabulary has a 50-dimensional word-embedding representation, and we have with a sentence containing 8 words, then $x$ would have a shape $(8, 50$) - representing that sentence numerically. Our model would produce $K$ classification scores for this input data.\n",
    "\n",
    "**The actual problem that we are solving is the following:**\n",
    "> Given a sequence of digits, return 1  if the first half and second half of a sequence are identical and 0 otherwise.\n",
    "\n",
    "We'll be using the following update equations for a simple RNN cell:\n",
    "<br/>\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\vec{h}_t = \\operatorname{ReLU}(\\vec{x}_t W_{xh} + \\vec{h}_{t-1} W_{hh} + \\vec{b}_h)$\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\vec{y}_t = \\vec{h}_t W_{hy} + \\vec{b}_y$\n",
    "\n",
    "where $\\vec{h}_t$ is the hidden (or recurrent) state of the cell and $\\vec{x}_t$ is the sequence-element at step-$t$, for $t=1, 2, \\dots, T$, where $T$ is the length of our sequence.\n",
    "The set $\\big(\\vec{y}_t\\big)_{t=1}^T$ is the output;\n",
    "_however_, for this particular problem, we will only use the classification scores from the final $y_T$.\n",
    "The $W_{xh}, W_{hh}, W_{hy}$ and $\\vec{b}_{h}, \\vec{b}_{y}$ parameters are the *learnable parameters of our model*. Specifically: \n",
    "\n",
    "- $\\vec{x}_t$ is a descriptor-vector for entry-$t$ in our sequence of data. It has a shape-$(1, C)$.\n",
    "- $\\vec{h}_t$ is a \"hidden-descriptor\", which encodes information about $\\vec{x}_t$ *and* information about the preceding entries in our sequence of data, via $\\vec{h}_{t-1}$. It has a shape-$(1, D)$, where $D$ is the dimensionality that we choose for our hidden descriptors (akin to layer size).\n",
    "- $W_{xh}$ and $\\vec{b}_h$ hold dense-layer weights and biases, respectively, which are used to process our data $\\vec{x}_t$ in order to form $\\vec{h}_t$. Thus $W_{xh}$ has shape $(C, D)$ and $\\vec{b}_h$ has shape-$(1,D)$. \n",
    "- $W_{hh}$ hold dense-layer weights, which are used to process our previous hidden-descriptor $\\vec{h}_{t-1}$ in order to form $\\vec{h}_t$. Thus $W_{hh}$ has shape $(D, D)$.\n",
    "- $W_{hy}$ and $\\vec{b}_y$ hold dense-layer weights and biases, respectively, which are used to process our hidden-descriptors $\\vec{h}_t$ in order to produce our classification scores, $y_t$. Thus $W_{hy}$ has shape $(D, K)$ and $\\vec{b}_h$ has shape-$(1,K)$, where $K$ is our number of classes. For this problem, given our input sequence $(\\vec{x}_t)_{t=1}^T$, we ultimately want to use the classification scores $y_T$ of shape-$(1, K)$.\n",
    "\n",
    "The basic idea is to have the forward pass in the model iterate over all elements in the input sequence, applying the update equations at each step.\n",
    "\n",
    "Then we'll compute the loss between the final output $y_T$ and the target classification, perform backpropagation through the computational graph to compute gradients (known as \"backpropagation through time\" or \"BPTT\" in RNNs), and update parameters using some form of gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150fa05",
   "metadata": {},
   "source": [
    "## Define Recurrent Model Class\n",
    "\n",
    "First create a recurrent model class using MyGrad and MyNN with the following properties:\n",
    "* `__init__`\n",
    " * Takes three parameters: dim_input ($C$), dim_recurrent ($D$), dim_output ($K$)\n",
    " * Creates three dense layers (required for update equations)\n",
    "  * Note: one of the dense layers doesn't need a bias since it would be redundant. You can specify `bias=False` when initializing your dense layer.\n",
    "  * You can leave the bias out of the dense layer corresponding to $W_{hh}$\n",
    "* `__call__`\n",
    " * If an initial hidden state $\\big(\\vec{h}_{t=0}\\big)$ is not provided, creates the initial hidden state as an array of zeros, shape-(1, D)\n",
    " * Iterates over the $T$-axis (rows) of the input sequence $(\\vec{x}_t)_{t=1}^T$ and computes and stores the successive hidden states $\\vec{h}_{t=1},\\; \\vec{h}_{t=2},\\; \\dots,\\; \\vec{h}_{t=T}$\n",
    " * After processing the all $T$ items in your sequence, computes the outputs $\\vec{y}_1,\\; \\vec{y}_2,\\; \\dots,\\; \\vec{y}_T$ and returns both the outputs and the hidden states as shape $(T,K)$ and $(T,D)$ tensors, respecitvely\n",
    "* `parameters`\n",
    " * Returns the tuple of all the learnable parameters in your model.\n",
    "\n",
    "As we ultimately want to use the final prediction scores $\\vec{y}_T$ in our loss, in our training loop we will need to extract this from the output of our `RNN` class. We will feed the shape `(1, K)` scores from $\\vec{y}_T$ to a softmax-crossentropy loss and so there is no need for an activation function on $\\vec{y}_T$, as softmax is built into the loss.\n",
    "\n",
    "Use `glorot_normal` for your dense weight initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \"\"\"Implements a simple-cell RNN that produces both outputs and hidden descriptors.\"\"\"\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all layers needed for RNN\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            Dimensionality of data passed to RNN (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            Dimensionality of hidden state in RNN (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            Dimensionality of output of RNN (K)\n",
    "        \"\"\"\n",
    "        # Initialize one dense layer for each matrix multiplication that appears\n",
    "        # in the simple-cell RNN equation; name these \"layers\" in ways that make\n",
    "        # their correspondence to the equation obvious\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    \n",
    "    def __call__(self, x, h=None):\n",
    "        \"\"\" Performs the full forward pass for the RNN.\n",
    "        \n",
    "        Note that we will return the hidden states h_t and classification scores y_t for the\n",
    "        full sequence, even though our loss will only utilize the last y_T.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, C)\n",
    "            The one-hot encodings for the sequence\n",
    "        \n",
    "        h: Optional[Union[numpy.ndarray, mygrad.Tensor]], shape=(1, D)\n",
    "            An optional initial hidden dimension state h_0.\n",
    "            If None, initialize an array of zeros.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[y, h]\n",
    "            y: mygrad.Tensor, shape=(T, K)\n",
    "                The final classification scores for each RNN step\n",
    "            h: mygrad.Tensor, shape=(T, D)\n",
    "                The hidden states computed at each RNN step, excluding the initial state h_0\n",
    "        \"\"\"\n",
    "        # Initialize the hidden state h_{t=0} as zeros if an\n",
    "        # initial hidden state is not provided as an argument.\n",
    "        #\n",
    "        # You will want to loop over each x_t to compute the\n",
    "        # corresponding h_t, then store each h_t in a list.\n",
    "        # You do not want to store the initial state h_{t=0}.\n",
    "        #\n",
    "        # You can use `mg.concatenate(list_of_h, axis=0)` to\n",
    "        # create a shape-(T, K) tensor of hidden-descriptors.\n",
    "        #\n",
    "        # A standard for-loop is appropriate here. Be mindful of what the shape \n",
    "        # of x_t should be versus the shape of the item that it produced by the\n",
    "        # for-loop.\n",
    "        #\n",
    "        # Note that you can do a for-loop over a mygrad-tensor and it will\n",
    "        # produce sub-tensors that are tracked by the computational graph.\n",
    "        # I.e. mygrad will be able to still \"backprop\" through your for-loop!\n",
    "        \n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503cd5c",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll apply this new network to the **problem of determining if a sequence of digits (0-9) is the concatentation of two identical halves.**\n",
    "\n",
    "For example:\n",
    "- `[1, 2, 3, 1, 2, 3]` -> contains identical halves\n",
    "- `[1, 9, 2, 1, 8, 3]` -> does not contain identical halves\n",
    "\n",
    "We will be representing each digit using the so-called \"**one-hot encoding**\"\n",
    " * 0 $\\longrightarrow$ [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " * 1 $\\longrightarrow$ [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " * 2 $\\longrightarrow$ [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " * 3 $\\longrightarrow$ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    " * $\\vdots$\n",
    " * 9 $\\longrightarrow$ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    " \n",
    "Thus a sequence of $T$ one-hot encoded digits will be represented by a shape-$(T,C=10)$ array. \n",
    "\n",
    "For example, the sequence\n",
    "```python\n",
    "# length-4 sequence\n",
    "array([2, 0, 2, 0])\n",
    "```\n",
    "Would have the one-hot encoding\n",
    "```python\n",
    "# shape-(4, 10)\n",
    "array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "```\n",
    "\n",
    "Create a function to generate a sample sequence that does the following:\n",
    "* allows you to specify min and max pattern length\n",
    "* randomly chooses a pattern length in the specified range\n",
    "* randomly generates a sequence of integers (0 through 9) of that length\n",
    "* sets first half of sequence equal to pattern\n",
    "* randomly chooses whether first and second half of sequence should match or not (with probability 0.5)\n",
    "* creates second half of sequence accordingly\n",
    "* creates float32 numpy array `x` of shape $(T, 10)$ where row i is one-hot encoding of item i in sequence\n",
    "* creates int16 numpy array `y` of shape $(1,)$ where `y = array([1])` if the patterns match and `array([0])` otherwise\n",
    "* returns `(x, y, sequence)` (note that sequence is returned mainly just for debugging)\n",
    "\n",
    "Note: `np.random.rand() < 0.5` returns `True` with 50% probability. This will come in handy!\n",
    "\n",
    "For example, if you randomly generate the sequence [2, 0, 2, 0] (which has a pattern-length of 2, whose first half does match the second half, which should occur 50% of the time), the output of your function should be:\n",
    "```python\n",
    "# x: one-hot encoded version of the sequence, shape-(4,10)\n",
    "array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])     \n",
    "\n",
    "# y: the halves of the sequence do match -> 1\n",
    "array([ 1])\n",
    "\n",
    "# sequence\n",
    "array([2, 0, 2, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a56dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(pattern_length_min=1, pattern_length_max=10, palindrome=False):\n",
    "    \"\"\"\n",
    "    Randomly generate a sequence consisting of two equal-length patterns of digits,\n",
    "    concatenated end-to-end. \n",
    "    \n",
    "    There should be a 50% chance that the two patterns are *identical* and a 50% \n",
    "    chance that the two patterns are distinct.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pattern_length_min : int, optional (default=1)\n",
    "       The smallest permissable length of the pattern (half the length of the \n",
    "       smallest sequence)\n",
    "       \n",
    "    pattern_length_max : int, optional (default=10)\n",
    "       The longest permissable length of the pattern (half the length of the \n",
    "       longest sequence)\n",
    "       \n",
    "    palindome : bool, optional (default=False)\n",
    "        If `True`, instead of a sequence with the two identical patterns, generate\n",
    "        a palindrome instead.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]\n",
    "        1. the one-hot encoded sequence; shape-(T, 10)\n",
    "        2. the label for the sequence: 0 (halves don't match), 1 (halves match); shape-(1,)\n",
    "        3. the actual sequence of digits; shape-(T,)\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7f8a4",
   "metadata": {},
   "source": [
    "Test your `generate_sequence` function manually.\n",
    "- Does it produce sequences within the desired length bounds?\n",
    "- Does `x` correspond to `sequence`, with the appropriate one-hot encoding?\n",
    "- Does `y` indicate `array([1])` when the halves of the sequence match?\n",
    "\n",
    "Consider writing some code with assert statements that will raise if any of these checks fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf52959",
   "metadata": {},
   "source": [
    "Set up a noggin plot, as you will want to observe the loss and accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163251f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509fe37",
   "metadata": {},
   "source": [
    "Recall that each digit has a one-hot encoding, which means that $C=10$ (`input_dim`). A sensible hidden-descriptor dimensionality is $D=50$ (`dim_recurrent`). Lastly, we are solving a *two-class* classification problem ($0\\rightarrow$ no pattern match, $1\\rightarrow$ pattern match), and thus $K=2$ (`dim_output`). Initialize your model accordingly.\n",
    "\n",
    "Set up an Adam optimizer. Pass the Adam optimizer your model's learnable parameters. Otherwise use its default learning rate and other hyperparameters. Feel free to mess with these later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f68b0",
   "metadata": {},
   "source": [
    "Train the model for 100000 iterations. Instead of pre-generating a set of training sequences, we'll use a strategy of randomly sampling a new input sequence every iteration using the method you created earlier. Use pattern_length_min = 1 and pattern_length_max = 10.\n",
    "\n",
    "**Do not plot batch-level metrics. We will be processing so many sequences, that plotting all the losses and accuracies will become a performance bottleneck**. You can set your loss and accuracy for each batch without plotting, using \n",
    "\n",
    "```python\n",
    "plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, \n",
    "                        batch_size=1, \n",
    "                        plot=False)\n",
    "```\n",
    "\n",
    "And then for every 500th batch (or whatever you want), call:\n",
    "\n",
    "```python\n",
    "plotter.set_train_epoch()\n",
    "```\n",
    "\n",
    "This will plot mean statistics for your model's performance instead of the accuracy and loss for every single input.\n",
    "\n",
    "**Take care to only pass in the final predicted value** $y_T$ **to the loss as a shape** $(1,K)$ **tensor**.\n",
    "\n",
    "After your training loop has completed one successful iteration, you can run `mg.turn_memory_guarding_off()`.\n",
    "This will speed up all subsequent iterations of your training loop.\n",
    "If you are interested to learn what this is all about, you can read about it [here](https://mygrad.readthedocs.io/en/latest/performance_tips.html#controlling-memory-guarding-behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b2241",
   "metadata": {},
   "source": [
    "### Accuracy vs Sequence Length\n",
    "\n",
    "Create a plot of accuracy vs sequence length. To do so, randomly generate sequences (which will be of various lengths), apply the trained model to get the predicted outputs, and record whether the model predictions are correct or not. Then compute accuracy for sequences of length 2, for sequences of length 4, etc. (hint: Keep track of total and total correct for each possible length).\n",
    "\n",
    "MyGrad note: Because you'll be evaluating the model on many sample sequences (`output = model(x)`), it's important to run this code in the `mg.no_autodiff` context:\n",
    "\n",
    "```python\n",
    "with mg.no_autodiff:\n",
    "   # autodiff is disabled here,\n",
    "   # thus mygrad will run faster\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4507bb2",
   "metadata": {},
   "source": [
    "What do you notice about accuracy as sequence length increases? Is this expected? What might make long sequences hard to deal with? Discuss with a neighbor!\n",
    "\n",
    "What happens if you apply the model to a sequence that's longer than examples it's been trained on? What happens if we train on and try to classify palindromes? Try messing around with our model and exploring the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca10d5b",
   "metadata": {},
   "source": [
    "### View the computational graph formed from processing a sequence\n",
    "\n",
    "We can view the computational graph that results from feeding a sequence through the RNN using MyGrad's awesome `build_graph` capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf29372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.computational_graph import build_graph\n",
    "x, target, sequence = generate_sequence()\n",
    "\n",
    "output, _ = model(x)\n",
    "output = output[-1:]\n",
    "\n",
    "loss = softmax_crossentropy(output, target)\n",
    "build_graph(loss, names=locals(), render=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-new_week3]",
   "language": "python",
   "name": "conda-env-.conda-new_week3-py"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
